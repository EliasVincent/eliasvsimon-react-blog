---
title: Vicunia - A chat GUI for local language models
author: Elias
date: April 10 2023
---

Have you ever wanted to chat with a powerful language model on your own device, without relying on cloud services or internet connection? If so, you might be interested in Vicunia!

<img src="/images/14.png" title="Vicunia" alt="Vicunia">

Alpaca.cpp is a project that allows you to run a fast ChatGPT-like model locally on your device. It combines the LLaMA foundation model with an open reproduction of Stanford Alpaca, a fine-tuning of the base model to act like ChatGPT and a set of modifications to llama.cpp to add a chat interface. Alpaca.cpp can run on Windows, Linux and macOS, and requires only 4GB of weights.

What is Vicunia?

Vicunia is a frontend for using alpaca.cpp and providing a GUI for installing and chatting with Stanford Alpaca. It is built with React.js, Next.js and Nextron. Vicunia aims to make it easy download and use models like Alpaca, without having to deal with CLI tools or complicated setup steps, which is how most local machine learning things are right now. Vicunia also offers some features such as emoji support, options menu for all parameters and exporting your chat history.

[Try it out here](https://github.com/EliasVincent/vicunia)

